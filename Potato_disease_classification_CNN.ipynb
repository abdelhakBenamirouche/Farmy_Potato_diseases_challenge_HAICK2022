{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da52423c-6c33-4c92-8102-47afa4441555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import cv2\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24868891-38f7-483c-940d-801c99a9d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN = \"C:/Users/SURFACE PRO/Desktop/3rd challenge/farmy-challenge-haick-2022/train/\"\n",
    "DIR_TEST = \"C:/Users/SURFACE PRO/Desktop/3rd challenge/farmy-challenge-haick-2022/test/\"\n",
    "DIR_VAL = \"C:/Users/SURFACE PRO/Desktop/3rd challenge/farmy-challenge-haick-2022/valid/\"\n",
    "\n",
    "DIR_TRAIN = pathlib.Path(DIR_TRAIN)\n",
    "DIR_VAL = pathlib.Path(DIR_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21488468-fef4-40e9-9cea-364802aad323",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = []\n",
    "noms = []\n",
    "for img in os.listdir(DIR_TEST):\n",
    "    noms.append(img)\n",
    "    test_imgs.append(load_img(DIR_TEST + img))\n",
    "#test_img = np.asarray(test_imgs)\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "dim = (img_width,img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331ef2b1-c793-419e-a404-0048da45a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2474 files belonging to 3 classes.\n",
      "Using 1980 files for training.\n",
      "Found 625 files belonging to 3 classes.\n",
      "Using 125 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"for img in test_imgs:\n",
    "    img = cv2.resize(img,dim)\"\"\"\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  DIR_TRAIN,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  DIR_VAL,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3a5509-d5b5-4afb-8b4f-e340f0a62030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 300, 300, 3)\n",
      "(32,)\n",
      "0.0 0.9527155\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "\n",
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c3667f-c69e-4835-ac98-9ebb5e6c13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768cb108-ff3c-4982-a068-122807040a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  data_augmentation,  \n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380001ef-0c8d-4246-a85f-e0d4925beb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad32549c-58fb-4dc2-832e-63cfd9c52f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "62/62 [==============================] - 111s 2s/step - loss: 1.3068 - accuracy: 0.5672 - val_loss: 0.8531 - val_accuracy: 0.6480\n",
      "Epoch 2/15\n",
      "62/62 [==============================] - 109s 2s/step - loss: 0.6835 - accuracy: 0.7283 - val_loss: 0.7021 - val_accuracy: 0.6800\n",
      "Epoch 3/15\n",
      "62/62 [==============================] - 110s 2s/step - loss: 0.5627 - accuracy: 0.7616 - val_loss: 0.5767 - val_accuracy: 0.6960\n",
      "Epoch 4/15\n",
      "62/62 [==============================] - 113s 2s/step - loss: 0.4730 - accuracy: 0.8091 - val_loss: 0.4374 - val_accuracy: 0.7920\n",
      "Epoch 5/15\n",
      "62/62 [==============================] - 115s 2s/step - loss: 0.4189 - accuracy: 0.8379 - val_loss: 0.3923 - val_accuracy: 0.8480\n",
      "Epoch 6/15\n",
      "62/62 [==============================] - 201s 3s/step - loss: 0.3992 - accuracy: 0.8449 - val_loss: 0.3911 - val_accuracy: 0.8640\n",
      "Epoch 7/15\n",
      "62/62 [==============================] - 471s 8s/step - loss: 0.3890 - accuracy: 0.8399 - val_loss: 0.4931 - val_accuracy: 0.7760\n",
      "Epoch 8/15\n",
      "62/62 [==============================] - 114s 2s/step - loss: 0.3672 - accuracy: 0.8475 - val_loss: 0.4094 - val_accuracy: 0.8080\n",
      "Epoch 9/15\n",
      "62/62 [==============================] - 110s 2s/step - loss: 0.3473 - accuracy: 0.8626 - val_loss: 0.3764 - val_accuracy: 0.8800\n",
      "Epoch 10/15\n",
      "62/62 [==============================] - 115s 2s/step - loss: 0.3324 - accuracy: 0.8626 - val_loss: 0.3544 - val_accuracy: 0.8640\n",
      "Epoch 11/15\n",
      "62/62 [==============================] - 135s 2s/step - loss: 0.3275 - accuracy: 0.8712 - val_loss: 0.3318 - val_accuracy: 0.8480\n",
      "Epoch 12/15\n",
      "62/62 [==============================] - 128s 2s/step - loss: 0.3135 - accuracy: 0.8722 - val_loss: 0.2434 - val_accuracy: 0.9120\n",
      "Epoch 13/15\n",
      "62/62 [==============================] - 116s 2s/step - loss: 0.2979 - accuracy: 0.8823 - val_loss: 0.3091 - val_accuracy: 0.8400\n",
      "Epoch 14/15\n",
      "62/62 [==============================] - 117s 2s/step - loss: 0.3007 - accuracy: 0.8763 - val_loss: 0.5178 - val_accuracy: 0.7520\n",
      "Epoch 15/15\n",
      "62/62 [==============================] - 117s 2s/step - loss: 0.2788 - accuracy: 0.8818 - val_loss: 0.2951 - val_accuracy: 0.8880\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f995c814-55e4-4c66-8078-452f082983bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "early blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "early blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "early blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "early blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "early blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "early blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "early blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "late blight\n",
      "early blight\n",
      "early blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "late blight\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "late blight\n",
      "healthy\n",
      "healthy\n",
      "early blight\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "i = 0\n",
    "for i in range(len(test_imgs)) :\n",
    "    img_array = img_to_array(test_imgs[i])\n",
    "    img_array = tf.image.resize(img_array, [300,300])\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    print(class_names[np.argmax(score)])\n",
    "    res.append(class_names[np.argmax(score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b0bd1f2-e33f-4c26-bf22-cf1843c2fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(noms,res)), columns=['image','Classification target'])\n",
    "df.to_csv('subs.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4f89b-ff98-49af-8dca-171a598b356d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f820490-9598-49e1-b3a2-aeaeef937058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72e47d-7200-4520-bee6-ff12c6f866ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74054f2-e95c-46c5-b15a-f9db47f2487d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefaf5c-d41b-49a4-8daa-2d85282ade8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ef472-1c6b-4e1b-a8fe-2a222b06ffaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
